{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FskZIVBOtXBQ"
      },
      "source": [
        "# 전통적인 머신러닝 방법을 사용한 텍스트 분류 파이프라인\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/corazzon/Mastering-NLP-from-Foundations-to-LLMs/refs/heads/main/cover.png'\n",
        "     alt=\"NLP와 LLM 실전 가이드(한빛미디어)\"\n",
        "     style=\"border: 3px solid gray; box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3); border-radius: 10px; width: 300px;\"   width='300'>\n",
        "\n",
        "\n",
        "* 저자:  \n",
        "    - [Lior Gazit](https://www.linkedin.com/in/liorgazit).  \n",
        "    - [Meysam Ghaffari](https://www.linkedin.com/in/meysam-ghaffari-ph-d-a2553088/).\n",
        "* 역자:\n",
        "    - [박조은](https://github.com/corazzon)\n",
        "* 이 노트북은 다음의 책에서 소개하는 내용입니다.\n",
        "    - 역서 : NLP와 LLM 실전 가이드(한빛미디어)\n",
        "    - 원서 : [Mastering NLP from Foundations to LLMs](https://www.amazon.com/dp/1804619183)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3waGSZ8Vm1A"
      },
      "source": [
        "colab 실습 :\n",
        "https://github.com/corazzon/Mastering-NLP-from-Foundations-to-LLMs\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/Mastering-NLP-from-Foundations-to-LLMs/blob/main/Chapter5_notebooks/Ch5_Text_Classification_Traditional_ML.ipynb)  \n",
        "\n",
        "\n",
        "원서 Colab 실습:   \n",
        "https://github.com/PacktPublishing/Mastering-NLP-from-Foundations-to-LLMs   \n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Mastering-NLP-from-Foundations-to-LLMs/blob/liors_branch/Chapter5_notebooks/Ch5_Text_Classification_Traditional_ML.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs04D5bdI8Mx"
      },
      "source": [
        ">*```면책사항: 이 노트북에서 다루는 내용과 아이디어는 저자들 개인의 것이며, 저자들의 고용주의 견해나 지적 재산을 대변하지 않습니다.```*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3Sw2NqA3_D0"
      },
      "source": [
        "**목표: 트윗이 \"기업 | 제품 뉴스\"를 논하고 있는지 식별하기 위한 트윗 처리.**\n",
        "이 노트북은 이진 분류기의 완전한 엔드-투-엔드 머신러닝 시스템 설계를 보여줍니다.\n",
        "트윗의 원문이 주어졌을 때, 분류기는 어떤 트윗이 기업이나 제품 뉴스를 논하고 있는지 식별합니다.\n",
        "\n",
        "## 파이프라인 구성:\n",
        "1. 코드 설정\n",
        "2. 데이터 수집\n",
        "3. 데이터 처리\n",
        "   1. 전처리\n",
        "   2. 예비 데이터 탐색\n",
        "   3. 특성 엔지니어링\n",
        "      1. 새로운 수치형 특성 탐색\n",
        "4. 훈련/테스트 분할\n",
        "5. 예비 통계 분석 및 실현 가능성 연구\n",
        "6. 특성 선택\n",
        "7. 머신러닝\n",
        "   1. ML 모델 반복\n",
        "   2. 선택된 모델 생성\n",
        "   3. 훈련 결과 생성\n",
        "   4. 테스트 결과 생성\n",
        "\n",
        "\n",
        "* 비고:\n",
        "이는 단일 노트북 파일에 완전히 포함되도록 설계된 완전한 머신러닝 파이프라인입니다. 이는 교육 도구로 사용됨을 의도하고 있습니다.\n",
        "전문적인 개발 환경에서는 재현성과 효율성을 위해 설계가 재현 가능한 `.py` 파일들로 작성되어야 합니다.\n",
        "\n",
        "## 데이터:\n",
        "[Hugging Face의 데이터셋:\n",
        "twitter-financial-news-topic](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic):\n",
        "\n",
        ">>\n",
        "**Twitter Financial News 데이터셋은 금융 관련 트윗의 주석이 달린 말뭉치를 포함하는 영어 데이터셋입니다. 이 데이터셋은 금융 관련 트윗의 주제를 분류하는 데 사용됩니다.**\n",
        "\n",
        "이 데이터셋은 20개의 레이블로 주석이 달린 21,107개의 문서를 포함합니다(참고로, 우리는 이 데이터셋의 레이블을 재지정하고 있습니다):\n",
        "```\n",
        "topics = {\n",
        "    \"LABEL_0\": \"애널리스트 업데이트\",\n",
        "    \"LABEL_1\": \"연준 | 중앙은행\",\n",
        "    \"**LABEL_2\": \"기업 | 제품 뉴스**\", (참고: 우리는 이 레이블에 집중할 것입니다)\n",
        "    \"LABEL_3\": \"국채 | 회사채\",\n",
        "    \"LABEL_4\": \"배당금\",\n",
        "    \"LABEL_5\": \"실적\",\n",
        "    \"LABEL_6\": \"에너지 | 석유\",\n",
        "    \"LABEL_7\": \"금융\",\n",
        "    \"LABEL_8\": \"통화\",\n",
        "    \"LABEL_9\": \"일반 뉴스 | 의견\",\n",
        "    \"LABEL_10\": \"금 | 금속 | 원자재\",\n",
        "    \"LABEL_11\": \"IPO\",\n",
        "    \"LABEL_12\": \"법률 | 규제\",\n",
        "    \"LABEL_13\": \"M&A | 투자\",\n",
        "    \"LABEL_14\": \"거시경제\",\n",
        "    \"LABEL_15\": \"시장\",\n",
        "    \"LABEL_16\": \"정치\",\n",
        "    \"LABEL_17\": \"인사이동\",\n",
        "    \"LABEL_18\": \"주식 논평\",\n",
        "    \"LABEL_19\": \"주가 움직임\",\n",
        "}\n",
        "```\n",
        "\n",
        "**데이터는 Twitter API를 사용하여 수집되었습니다. 현재 데이터셋은 다중 클래스 분류 작업을 지원합니다.**\n",
        "\n",
        "**요구사항:**\n",
        "* Colab에서 실행할 때는 다음 런타임 노트북 설정을 사용하세요: `Python3, CPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g54Uf66Vz9Fi"
      },
      "source": [
        ">*```면책사항: 이 노트북에서 다루는 내용과 아이디어는 저자들 개인의 것이며, 저자들의 고용주의 견해나 지적 재산을 대변하지 않습니다.```*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ1LYgXOpDJ6"
      },
      "source": [
        "설치 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO5KM7JCpDUP"
      },
      "outputs": [],
      "source": [
        "# 비고(REMARK):\n",
        "# 아래 코드가 Python 패키지 불일치로 인해 오류가 발생할 경우, 이는 새로운 버전의 업데이트로 인해 발생했을 가능성이 있습니다.\n",
        "# 이러한 경우, \"default_installations\" 값을 False로 설정하여 기본 이미지를 복원하십시오:\n",
        "default_installations = True\n",
        "\n",
        "if default_installations:\n",
        "    !pip -q install datasets num2words autocorrect\n",
        "else:\n",
        "    import requests\n",
        "    text_file_path = \"requirements__Ch5_Text_Classification_Traditional_ML.txt\"\n",
        "    url = \"https://raw.githubusercontent.com/PacktPublishing/Mastering-NLP-from-Foundations-to-LLMs/main/Chapter5_notebooks/\" + text_file_path\n",
        "    res = requests.get(url)\n",
        "    with open(text_file_path, \"w\") as f:\n",
        "        f.write(res.text)\n",
        "\n",
        "    !pip install -r requirements__Ch5_Text_Classification_Traditional_ML.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_--320f-8Yi"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0zHJoN61Zu5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "import scipy\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "from num2words import num2words\n",
        "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from autocorrect import Speller\n",
        "\n",
        "# 머신러닝 관련\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
        "import sklearn.linear_model as lm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql6clhha--CD"
      },
      "source": [
        "### Code Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02lniQAu-6ou"
      },
      "outputs": [],
      "source": [
        "# 코드 설정:\n",
        "# db_name: 원본 데이터를 보유한 HuggingFace 데이터베이스 이름\n",
        "# do_preprocessing: 논리값, 데이터 전처리를 수행할지 여부\n",
        "# do_enhanced_preprocessing: 논리값, 연산량이 큰 전처리를 수행할지 여부\n",
        "# do_feature_eng: 논리값, 특성 공학을 수행할지 여부\n",
        "# maximize_a_priori: 논리값, 사전 확률(a priori) 또는 사후 확률(a posteriori) 통계에 기반하여 초기 특성 선택을 최대화할지 여부\n",
        "# num_chosen_features_per_class: 정수값, 초기 특성 선택 시 클래스별로 선택할 특성의 수\n",
        "# test_size: 0에서 1 사이의 비율, 테스트 데이터셋 크기를 나타냄\n",
        "# feature_eng_details: \"TfidfVectorizer\"(TF-IDF 특성 공학) 또는 \"CountVectorizer\"(원-핫 인코딩 중 하나)를 선택\n",
        "# seed: 정수값, 결과의 재현성을 보장하기 위해 사용하는 랜덤 시드\n",
        "config_dict = {'db_name': \"zeroshot/twitter-financial-news-topic\",\n",
        "               'do_preprocessing': True,\n",
        "               'do_enhanced_preprocessing': False,\n",
        "               'do_feature_eng': True,\n",
        "               'maximize_a_priori': True,\n",
        "               'num_chosen_features_per_class': 200,\n",
        "               'test_size': 0.2,\n",
        "               'feature_eng_details': \"CountVectorizer-binary\",\n",
        "               'ngram_range_min': 1,\n",
        "               'ngram_range_max': 2,\n",
        "               'max_features': 1000,\n",
        "               'seed': 0}\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d6x7m9s_eh0"
      },
      "source": [
        "### 데이터 수집"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJd1W79I4JmR"
      },
      "outputs": [],
      "source": [
        "dataset_raw = load_dataset(config_dict[\"db_name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utLjs7A60MSQ"
      },
      "source": [
        "### 데이터 처리  \n",
        "머신러닝 모델을 학습시키고 예측하기 위해 데이터를 설정하는 단계입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9aEAKgM7yUB"
      },
      "source": [
        "하나의 완전한 데이터프레임을 생성합니다.  \n",
        "HuggingFace는 원래 데이터셋을 훈련(train)과 검증(validation) 두 개의 하위 집합으로 나누어 제공합니다.  \n",
        "우리는 이를 하나로 병합한 후, 나중에 원하는 비율로 다시 분할할 예정입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wi1mAY7yeO"
      },
      "outputs": [],
      "source": [
        "first_df = pd.DataFrame(dataset_raw[\"train\"])\n",
        "second_df = pd.DataFrame(dataset_raw[\"validation\"])\n",
        "dataset_df = pd.concat([first_df, second_df]).reset_index(drop=True)\n",
        "dataset_df = dataset_df.rename(columns={\"label\": \"_label_\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOo5mBx004k7"
      },
      "source": [
        "원본 데이터를 일부 살펴봅니다:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5wF9r9A04uA"
      },
      "outputs": [],
      "source": [
        "dataset_df.head(10).style.set_properties(**{'text-align': 'left'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ7hseIDJbX0"
      },
      "outputs": [],
      "source": [
        "print(\"원본 레이블 분포:\\n\")\n",
        "dataset_df[[\"_label_\"]].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJIfEYzYzzC4"
      },
      "source": [
        "우리는 특정 주제인 **\"회사 | 제품 뉴스\"**(레이블 2)에 초점을 맞출 것입니다.  \n",
        "따라서 레이블을 다음과 같이 재분류할 예정입니다:  \n",
        ">>\n",
        "- 레이블 0: **회사 | 제품 뉴스가 아닌 경우**  \n",
        "- 레이블 1: **회사 | 제품 뉴스인 경우**  \n",
        "\n",
        "이제 분류 문제는 이진 분류 문제로 전환됩니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbe_tsjmzzMX"
      },
      "outputs": [],
      "source": [
        "dataset_df_binary = dataset_df.copy()\n",
        "dataset_df_binary[\"_label_\"] = dataset_df_binary[\"_label_\"].map({2:1}).fillna(0).map(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atVRw5FFIMaJ"
      },
      "outputs": [],
      "source": [
        "print(\"새로운 레이블의 분포:\\n\")\n",
        "frequencies = dataset_df_binary[[\"_label_\"]].value_counts()\n",
        "frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq9QEVWXbTMX"
      },
      "outputs": [],
      "source": [
        "most_frequent_class = frequencies.index[:][0][0]\n",
        "print(\"가장 빈도가 높은 클래스는:\", most_frequent_class)\n",
        "print(\"해당 클래스의 기준 정확도는:\", round((dataset_df_binary[\"_label_\"] == most_frequent_class).mean(), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5KVvs7J_giZ"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJm8NNust0H0"
      },
      "source": [
        "전처리 유틸리티 함수 정의:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNJaNvEYt0SM"
      },
      "outputs": [],
      "source": [
        "def digits_to_words(match):\n",
        "    \"\"\"\n",
        "    문자열로 된 숫자를 영어 단어로 변환합니다. 이 함수는 기수(cardinal)와 서수(ordinal)를 구분합니다.\n",
        "    예: \"2\" → \"two\", \"2nd\" → \"second\"\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    suffixes = ['st', 'nd', 'rd', 'th']\n",
        "    # 이전 작업에 의존하지 않도록 소문자로 변환:\n",
        "    string = match[0].lower()\n",
        "    if string[-2:] in suffixes:\n",
        "        type = 'ordinal'\n",
        "        string = string[:-2]\n",
        "    else:\n",
        "        type = 'cardinal'\n",
        "\n",
        "    return num2words(string, to=type)\n",
        "\n",
        "\n",
        "def spelling_correction(text):\n",
        "    \"\"\"\n",
        "    잘못된 철자를 올바른 철자로 교정합니다.\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    corrector = Speller()\n",
        "    spells = [corrector(word) for word in text.split()]\n",
        "    return \" \".join(spells)\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    불용어(stopwords)를 제거합니다.\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    return \" \".join([word for word in text.split() if word not in stopwords_set])\n",
        "\n",
        "\n",
        "def stemming(text):\n",
        "    \"\"\"\n",
        "    각 단어에 대해 어간 추출(stemming)을 수행합니다.\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def lemmatizing(text):\n",
        "    \"\"\"\n",
        "    각 단어에 대해 표제어 추출(lemmatization)을 수행합니다.\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def preprocessing(input_text):\n",
        "    \"\"\"\n",
        "    텍스트 전처리를 위한 전체 파이프라인을 나타냅니다.\n",
        "\n",
        "    입력: str\n",
        "    출력: str\n",
        "    \"\"\"\n",
        "    output = input_text\n",
        "    # 소문자로 변환:\n",
        "    output = output.lower()\n",
        "    # URL 제거:\n",
        "    output = re.sub(r'http\\S+', \"\", output)\n",
        "    # 숫자를 단어로 변환:\n",
        "    # 다음 정규 표현식은 연속된 숫자와 임시적으로 서수 접미사를 포함한 일치를 검색합니다:\n",
        "    output = re.sub(r'\\d+(st)?(nd)?(rd)?(th)?', digits_to_words, output, flags=re.IGNORECASE)\n",
        "    # 구두점 및 기타 특수 문자 제거:\n",
        "    output = re.sub('[^ A-Za-z0-9]+', '', output)\n",
        "\n",
        "    if config_dict[\"do_enhanced_preprocessing\"]:\n",
        "        # 철자 교정:\n",
        "        output = spelling_correction(output)\n",
        "\n",
        "    # 불용어 제거:\n",
        "    output = remove_stop_words(output)\n",
        "\n",
        "    if config_dict[\"do_enhanced_preprocessing\"]:\n",
        "        # 어간 추출:\n",
        "        output = stemming(output)\n",
        "        # 표제어 추출:\n",
        "        output = lemmatizing(output)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgrWf0mEt0bm"
      },
      "source": [
        "전처리 수행:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEedfizM6FtZ"
      },
      "outputs": [],
      "source": [
        "dataset_clean = dataset_df_binary.copy()\n",
        "if config_dict[\"do_preprocessing\"]:\n",
        "    dataset_clean[\"text\"] = [preprocessing(text) for text in dataset_clean[\"text\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSYLrwr7KREy"
      },
      "outputs": [],
      "source": [
        "dataset_clean.head(10).style.set_properties(**{'text-align': 'left'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht259WodQFLK"
      },
      "source": [
        "## 초기 데이터 탐색  \n",
        "\n",
        "모든 머신러닝 프로젝트는 데이터를 기본적으로 탐색하는 작업에서 시작해야 합니다.  \n",
        "그 주요 목표는 데이터의 특성을 파악하고, \"X\" 데이터(이 경우, 트윗 텍스트)와 원하는 값 \"Y\"(이 경우, 주제 분류) 간의 연관성을 연구하는 것입니다.  \n",
        "\n",
        "우선, 데이터의 가장 단순한 특성 중 하나인 각 트윗의 길이를 살펴보는 것부터 시작하겠습니다.  \n",
        "이후에는 사용된 언어와 주제 레이블 간의 통계적 의존성을 탐구할 예정입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpmqTsLitdYy"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq koreanize-matplotlib\n",
        "import koreanize_matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbe9QF-lNnLs"
      },
      "outputs": [],
      "source": [
        "dataset_clean[\"length of text\"] = dataset_clean[\"text\"].map(len)\n",
        "ax = dataset_clean.plot.hist(column=[\"length of text\"], by=\"_label_\", bins=50, alpha=0.5, figsize=(10, 6),\n",
        "                             title=\"클래스별 트윗 문자열 길이 분포\", xlim=[0, 1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgDwJos4FgJK"
      },
      "source": [
        "## 특성 공학"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX9IFcHYFi7f"
      },
      "outputs": [],
      "source": [
        "def feat_eng_text_df(in_df, text_col, labels_col, config_dict):\n",
        "    if \"CountVectorizer-binary\" == config_dict[\"feature_eng_details\"]:\n",
        "        # 특성 공학 방법: 이진 (원-핫 인코딩)\n",
        "        print(\"특성 공학 방법: 이진 (원-핫 인코딩)\")\n",
        "        countvectorizer = CountVectorizer(ngram_range=(config_dict[\"ngram_range_min\"], config_dict[\"ngram_range_max\"]),\n",
        "                                          stop_words='english',\n",
        "                                          max_features=config_dict[\"max_features\"],\n",
        "                                          binary=True)\n",
        "\n",
        "    elif \"CountVectorizer-BOW\" == config_dict[\"feature_eng_details\"]:\n",
        "        # 특성 공학 방법: 단어 가방(Bag of Words)\n",
        "        print(\"특성 공학 방법: 단어 가방(Bag of Words)\")\n",
        "        countvectorizer = CountVectorizer(ngram_range=(config_dict[\"ngram_range_min\"], config_dict[\"ngram_range_max\"]),\n",
        "                                          stop_words='english',\n",
        "                                          max_features=config_dict[\"max_features\"],\n",
        "                                          binary=False)\n",
        "\n",
        "    out_arr = countvectorizer.fit_transform(in_df[text_col])\n",
        "    count_tokens = countvectorizer.get_feature_names_out()\n",
        "    out_df = pd.DataFrame(data=out_arr.toarray(), columns=count_tokens)\n",
        "    out_df[labels_col] = list(in_df[labels_col])\n",
        "    return out_df\n",
        "\n",
        "\n",
        "if config_dict[\"do_feature_eng\"]:\n",
        "    dataset_feat_eng = feat_eng_text_df(dataset_clean, 'text', '_label_', config_dict)\n",
        "else:\n",
        "    # 주의: 특성 공학을 건너뛰는 옵션입니다.\n",
        "    # 이 옵션은 BERT, GPT 등 자체적으로 텍스트를 처리하는\n",
        "    # 딥러닝 언어 모델을 사용할 때만 선택하세요.\n",
        "    # 일반적인 머신러닝 모델 사용시 특성 공학은 필수입니다.\n",
        "    dataset_feat_eng = dataset_clean.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_WMzIFM_i6A"
      },
      "source": [
        "## 새로운 수치형 특성 탐색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqz7oNbE_328"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZPYzjltUZ3Q"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng.describe().loc[['min', 'max', 'mean']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf-JsLSKC-Vk"
      },
      "source": [
        "## Train/Test 데이터 세트 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQzV5OeY6RaZ"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng_test = dataset_feat_eng.sample(frac=config_dict[\"test_size\"],random_state=config_dict['seed'])\n",
        "dataset_feat_eng_train = dataset_feat_eng.drop(dataset_feat_eng_test.index)\n",
        "dataset_feat_eng_test.shape, dataset_feat_eng_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfsphIcZFZEn"
      },
      "source": [
        "## 초기 통계 분석 및 실현 가능성 연구\n",
        "\n",
        "이 과정은 머신러닝을 적용하기 전에 수행하는 초기 연구에서 가장 가치 있는 단계 중 하나입니다.  \n",
        "이 단계에서는 \"X\"와 \"Y\" 간의 관계를 측정하여 \"상관관계\"가 존재하는지 확인합니다.\n",
        "\n",
        "만약 X와 Y가 모두 수치 데이터이고 회귀 문제라면, X와 Y 간의 상관관계를 평가하여 선형 회귀 모델이 좋은 결과를 낼 가능성을 확인하는 것이 합리적일 것입니다.\n",
        "\n",
        "그러나 X와 Y 모두 본질적으로 수치 데이터가 아니므로, 두 변수 간의 **통계적 의존성**을 평가하여 모델이 탐지할 수 있는 \"신호\"가 존재하는지 판단하려고 합니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ZtgfrJ9nhl"
      },
      "source": [
        "계산:  \n",
        "**P(feature | class)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPvdzu9utqMC"
      },
      "outputs": [],
      "source": [
        "# 클래스별 특성 통계:\n",
        "means_by_class = dataset_feat_eng_train.groupby(by=[\"_label_\"]).mean().T.sort_index()\n",
        "means_by_class.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LINyT4DBAsf8"
      },
      "source": [
        "통계적 의존성을 반영하는 비율을 계산합니다:  \n",
        "**P(class, feature)/(P(class)P(feature))**  \n",
        "이것은 다음과 같이 다시 쓸 수 있습니다:  \n",
        "**P(class | feature)/P(class)**  \n",
        "또는 동등하게:  \n",
        "**P(feature | class)/P(feature)**  \n",
        "\n",
        "\\*참고:  \n",
        "아래 계산은 각 텍스트 항목의 수치적 특성이 **이진(binary)**인 경우에만 확률 측정으로 간주됩니다.  \n",
        "만약 Bag of Words(BOW)나 TF/IDF와 같은 다른 특성 방법을 사용한다면, 아래 값은 확률이 아니라 그 대략적인 값을 나타냅니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QwHZPJwuAssf",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "P_class = sorted([[c, np.mean(dataset_feat_eng[\"_label_\"] == c)] for c in set(means_by_class.columns)])\n",
        "P_feature = sorted([[f, np.mean(dataset_feat_eng[f] > 0)] for f in dataset_feat_eng.columns if f != \"_label_\"])\n",
        "P_feature_inv = [[f, 1/p] for f, p in P_feature]\n",
        "\n",
        "P_class_arr = np.array(P_class)\n",
        "P_feature_arr = np.array(P_feature)\n",
        "P_feature_inv_arr = np.array(P_feature_inv)\n",
        "# 특성 확률의 \"열 벡터\"와 클래스 확률의 \"행 벡터\"를 곱하여\n",
        "# 각 요소가 확률의 곱으로 구성된 행렬을 생성합니다.\n",
        "P_class_prod_P_feature_inv_arr = np.outer(P_feature_inv_arr[:, 1].astype(float), P_class_arr[:, 1].astype(float))\n",
        "\n",
        "P_class_given_feature = means_by_class.copy()\n",
        "for feature_counter in range(len(P_class_given_feature)):\n",
        "  for c in P_class_given_feature.columns:\n",
        "    # 오른쪽 항: P(feature | class) / P(feature)\n",
        "    P_class_given_feature[c][feature_counter] = means_by_class[c][feature_counter] / P_feature_arr[feature_counter, 1].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipfL9o6XX8pE"
      },
      "source": [
        "**클래스 \"0\"을 가장 잘 나타내는 단어들:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEl4iKwaX82G"
      },
      "outputs": [],
      "source": [
        "P_class_given_feature.sort_values([0], ascending=False)[[0]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZFPdu0ZZgW"
      },
      "source": [
        "**클래스 \"1\"을 가장 잘 나타내는 단어들:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqyvb9KlZZyW"
      },
      "outputs": [],
      "source": [
        "P_class_given_feature.sort_values([1], ascending=False)[[1]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyHadLWYchW7"
      },
      "source": [
        "## 특성 선택\n",
        "\n",
        "이 과정은 단변량 특성 선택(univariate feature selection)입니다.  \n",
        "특성 값이 0/1일 때와 클래스 값이 0/1일 때의 조건부 의존성을 기반으로 하며, 특성의 평균 값이 해당 특성의 확률을 나타냅니다.  \n",
        "특성 선택 과정은 **훈련 데이터셋에서만** 수행된다는 점에 유의하세요.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glryYgkh9uGr"
      },
      "source": [
        "각 클래스에 대해 가장 특징적인 특성을 선택합니다.  \n",
        "다음 중 하나를 극대화합니다:  \n",
        "* 사전 확률 분포 $ P(\\text{feature} | \\text{class})$, 최대 가능도(Max Likelihood)  \n",
        "또는  \n",
        "* 사후 확률 $ P(\\text{class} | \\text{feature})$, 최대 사후 확률(MAP)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTnI3Lpk95BU"
      },
      "outputs": [],
      "source": [
        "chosen_features = []\n",
        "if config_dict[\"maximize_a_priori\"] == True:\n",
        "    classes = means_by_class.columns\n",
        "    for c in classes:\n",
        "        chosen_features += list(means_by_class[c].sort_values(ascending=False).index[:config_dict[\"num_chosen_features_per_class\"] + 1])\n",
        "else:\n",
        "    classes = P_class_given_feature.columns\n",
        "    for c in classes:\n",
        "        chosen_features += list(P_class_given_feature[c].sort_values(ascending=False).index[:config_dict[\"num_chosen_features_per_class\"] + 1])\n",
        "\n",
        "chosen_features = list(set(chosen_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsRkvKXuAApE"
      },
      "outputs": [],
      "source": [
        "chosen_features[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhgOQmjkBq78"
      },
      "source": [
        "### 선택된 특성만 남기기:  \n",
        "훈련 데이터를 기반으로 어떤 특성이 \"중요한지\"를 도출했으므로, 이 특성들을 훈련 데이터와 테스트 데이터 모두에 적용하여 선택합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4bY4gmWBrGe"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng_train_selected = dataset_feat_eng_train.filter(chosen_features + [\"_label_\"])\n",
        "dataset_feat_eng_test_selected = dataset_feat_eng_test.filter(chosen_features + [\"_label_\"])\n",
        "\n",
        "dataset_feat_eng_train_selected.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jiW-llRT4VK"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng_train_selected[\"_label_\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JAQspmJCwfT"
      },
      "source": [
        "# 머신러닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIVm-heY6AdA"
      },
      "outputs": [],
      "source": [
        "dataset_feat_eng_train_selected.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffIH343jLKRs"
      },
      "source": [
        "데이터셋에서 Y 레이블을 추출하고, 모델에 적합하도록 변수 유형을 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox0X7mDgLKg1"
      },
      "outputs": [],
      "source": [
        "x_features_train = dataset_feat_eng_train_selected.values[:, 0:-1]\n",
        "y_labels_train = dataset_feat_eng_train_selected.values[:, -1]\n",
        "\n",
        "x_features_test = dataset_feat_eng_test_selected.values[:, :-1]\n",
        "y_labels_test = dataset_feat_eng_test_selected.values[:, -1]\n",
        "\n",
        "x_features_train.shape, y_labels_train.shape, x_features_test.shape, y_labels_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDAccMSXDD7N"
      },
      "source": [
        "#### 머신러닝 모델 반복 실험  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJyaQVzyDETH"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "models.append((\"Random Forest\", RandomForestClassifier(random_state=config_dict['seed'])))\n",
        "models.append((\"LASSO\", lm.LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000, random_state=config_dict['seed'])))\n",
        "models.append((\"KNN\", KNeighborsClassifier()))\n",
        "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=config_dict['seed'])))\n",
        "models.append((\"SVM\", SVC(gamma='auto', random_state=config_dict['seed'])))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "best_mean_result = 0\n",
        "best_std_result = 0\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold()\n",
        "    cv_results = cross_val_score(model, x_features_train, y_labels_train, scoring='accuracy', cv=kfold)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    print(name + \": mean(accuracy)=\" + str(round(np.mean(cv_results), 3)) + \", std(accuracy)=\" + str(round(np.std(cv_results), 3)))\n",
        "    if (best_mean_result < np.mean(cv_results)) or \\\n",
        "        ((best_mean_result == np.mean(cv_results)) and (best_std_result > np.std(cv_results))):\n",
        "        best_mean_result = np.mean(cv_results)\n",
        "        best_std_result = np.std(cv_results)\n",
        "        best_model_name = name\n",
        "        best_model = model\n",
        "print(\"\\n최적의 모델은:\\n\" + best_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HUN7jJE1IcP"
      },
      "source": [
        "검증 폴드별 결과 분포를 관찰합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx1tB-Vw1IuI"
      },
      "outputs": [],
      "source": [
        "matplotlib.pyplot.boxplot(results, labels=names)\n",
        "matplotlib.pyplot.title(\"Models' results' distribution of accuracy\")\n",
        "matplotlib.pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6HrRE6U1YBS"
      },
      "source": [
        "### 선택한 모델 생성  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oODOQLuOoFc2"
      },
      "source": [
        "하이퍼파라미터 최적화:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQhKbQgrqVbP"
      },
      "outputs": [],
      "source": [
        "model = lm.LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000, random_state=config_dict['seed'])\n",
        "params = {\"C\": np.linspace(start=0.001, stop=10, num=20)}\n",
        "grid_search = GridSearchCV(model, params, scoring='accuracy')\n",
        "grid_search.fit(x_features_train, y_labels_train)\n",
        "print(\"최적의 하이퍼파라미터 'C' 값은:\", grid_search.best_params_[\"C\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjK2e9GaoO34"
      },
      "source": [
        "최적화된 모델을 훈련 데이터셋에 적합합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC8baVMOoPHk"
      },
      "outputs": [],
      "source": [
        "model = lm.LogisticRegression(C=grid_search.best_params_[\"C\"], max_iter=1000, random_state=config_dict['seed'])\n",
        "model.fit(x_features_train, y_labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I237S_fTDEqj"
      },
      "outputs": [],
      "source": [
        "# These two cells are for if we wanted to pick the Random Forest model:\n",
        "\n",
        "# model = RandomForestClassifier()\n",
        "# params = {\"n_estimators\": range(10, 31, 3),\n",
        "#           \"min_samples_split\": range(2, 10, 2)}\n",
        "# grid_search = GridSearchCV(model, params)\n",
        "# grid_search.fit(x_features_train, y_labels_train)\n",
        "\n",
        "# print(\"The optimal hyperparameter 'n_estimators' is:\", grid_search.best_params_[\"n_estimators\"])\n",
        "# print(\"The optimal hyperparameter 'min_samples_split' is:\", grid_search.best_params_[\"min_samples_split\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuInjrRpPIK4"
      },
      "outputs": [],
      "source": [
        "# model = RandomForestClassifier(n_estimators=grid_search.best_params_[\"n_estimators\"],\n",
        "#                                min_samples_split=grid_search.best_params_[\"min_samples_split\"],\n",
        "#                                )\n",
        "# model.fit(x_features_train, y_labels_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZinbMBQb8t5D"
      },
      "source": [
        "### 훈련 결과 생성: 설계 선택에 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1A1nr-kSS0q"
      },
      "outputs": [],
      "source": [
        "y_train_estimated = model.predict(x_features_train)\n",
        "accuracy_train = np.mean(y_train_estimated == y_labels_train)\n",
        "baseline_accuracy_train = np.mean(0 == y_labels_train)\n",
        "accuracy_lift_train = 100 * (accuracy_train/baseline_accuracy_train - 1)\n",
        "\n",
        "print(\"훈련 세트 결과:\\n-------------------------\")\n",
        "print(\"기준 모델(더미 분류기) 정확도:\", round(baseline_accuracy_train, 2))\n",
        "print(\"현재 모델의 정확도:\", round(accuracy_train, 2))\n",
        "print(\"정확도 향상은:\", round(accuracy_lift_train), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRSrEo2j8y0P"
      },
      "source": [
        "### 테스트 결과 생성: 성능 설명에 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msAAgYoBSiK3"
      },
      "outputs": [],
      "source": [
        "y_test_estimated = model.predict(x_features_test)\n",
        "accuracy_test = np.mean(y_test_estimated == y_labels_test)\n",
        "baseline_accuracy_test = np.mean(0 == y_labels_test)\n",
        "accuracy_lift = 100 * (accuracy_test/baseline_accuracy_test - 1)\n",
        "\n",
        "print(\"테스트 세트 결과:\\n-------------------------\")\n",
        "print(\"기준 모델(더미 분류기) 정확도:\", round(baseline_accuracy_test, 2))\n",
        "print(\"현재 모델의 정확도:\", round(accuracy_test, 2))\n",
        "print(\"정확도 향상은:\", round(accuracy_lift), \"%\")\n",
        "\n",
        "print(\"\\n혼동 행렬(Confusion Matrix):\")\n",
        "print(confusion_matrix(y_labels_test, y_test_estimated))\n",
        "print(\"\\n분류 보고서(Classification Report):\")\n",
        "print(classification_report(y_labels_test, y_test_estimated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE8BwJGJDEmD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}